{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sudarshan-koirala/langchain-falcon-chainlit/blob/main/langchain_falcon.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install langchain huggingface_hub watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Guilherme SÃ¡\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "langchain      : 0.0.190\n",
      "huggingface_hub: 0.15.1\n",
      "\n",
      "Compiler    : GCC 9.4.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-1040-azure\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext watermark\n",
    "%watermark -a \"Guilherme SÃ¡\" -vmp langchain,huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your Huggingface access token from https://huggingface.co/settings/tokens ðŸ”‘\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "HUGGINGFACE_API_TOKEN = getpass()\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_TOKEN   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use falcon-7b-instruct model from [Huggingface website](https://huggingface.co/tiiuae/falcon-7b-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token=HUGGINGFACE_API_TOKEN, \n",
    "                     repo_id=repo_id, \n",
    "                     model_kwargs={\"temperature\":0.7, \"max_new_tokens\":700})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Prepare your pizza dough by kneading it until it becomes smooth and elastic.\n",
      "2. Roll the dough out to your desired thickness on a floured surface.\n",
      "3. Transfer the dough onto a baking sheet or pizza stone.\n",
      "4. Preheat your oven to the specified temperature.\n",
      "5. Add your choice of sauce, cheese, and toppings onto the rolled-out dough.\n",
      "6. Place the baking sheet or pizza stone in the preheated oven and bake the pizza.\n",
      "7. Check periodically to ensure even baking and to prevent burning.\n",
      "8. Once the crust and cheese are golden brown, remove the pizza from the oven.\n",
      "9. Allow the pizza to cool for a few minutes before slicing and serving.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant and provide the answer for the question asked politely.\n",
    "\n",
    "{question}\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How to cook Pizza ?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
